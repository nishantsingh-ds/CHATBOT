{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a41b3e",
   "metadata": {},
   "source": [
    "# Hotel Reservation Chatbot using Custom Simple Recurrent Neural Network (RNN)\n",
    "\n",
    "This project aims to create an intelligent hotel reservation chatbot using a **Simple Recurrent Neural Network (RNN)**, implemented from scratch. The chatbot interacts with users, understands their intentions from their inputs, and responds accordingly based on the **intents.json** dataset.\n",
    "\n",
    "### **Project Overview:**\n",
    "\n",
    "The **Hotel Reservation Chatbot** leverages a custom RNN model built with the `SimpleRecurrentNeuralNetwork()` class to predict the user's intent, allowing the bot to provide appropriate responses related to hotel reservations. The dataset used for training contains a variety of patterns and corresponding tags, such as booking requests, check-in inquiries, and other reservation-related dialogues.\n",
    "\n",
    "### **Key Features:**\n",
    "\n",
    "- **Custom SimpleRNN Model:** This project employs a custom RNN architecture, `SimpleRecurrentNeuralNetwork()`, to handle sequential data and make predictions based on past inputs (user queries).\n",
    "  \n",
    "- **Intent Recognition:** The chatbot classifies user queries into predefined intent categories (e.g., booking, check-in, cancellation), ensuring that the responses are contextually relevant.\n",
    "\n",
    "- **Hotel Reservation Dataset:** The intents and patterns used for training are sourced from a JSON file, `intents.json`, containing examples of user interactions in the context of hotel reservations.\n",
    "\n",
    "- **Training & Testing:** The model is trained on the dataset, learning to map patterns (user inputs) to corresponding tags (intents), and is then tested for accurate predictions.\n",
    "\n",
    "### **How it Works:**\n",
    "\n",
    "1. **User Input Processing:** The chatbot tokenizes and pads the user input to create a uniform input sequence suitable for the RNN.\n",
    "2. **Prediction:** The RNN model predicts the most likely intent based on the processed input.\n",
    "3. **Response Generation:** Once the intent is predicted, a relevant response from the dataset is retrieved and sent to the user.\n",
    "\n",
    "This project provides a foundational example of how a custom RNN can be utilized to build an interactive and functional chatbot for tasks like hotel reservations, making it easier for users to inquire about room availability, bookings, cancellations, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ceb75",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# SimpleRNN: Implementation of a Recurrent Neural Network\n",
    "\n",
    "This part of notebook demonstrates a custom implementation of a simple Recurrent Neural Network (RNN) from scratch in Python. The `SimpleRNN` class is designed to handle basic sequential data tasks such as sequence prediction. The code is modularized into several functions for initialization, forward propagation, backward propagation, training, and prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Class Overview: `SimpleRNN`\n",
    "\n",
    "The `SimpleRNN` class is structured as follows:\n",
    "\n",
    "### **1. Initialization (`__init__`)**\n",
    "The constructor initializes the RNN's parameters:\n",
    "- **Inputs:**\n",
    "  - `input_size`: Number of features in each input vector.\n",
    "  - `hidden_size`: Number of units in the hidden layer.\n",
    "  - `output_size`: Number of features in the output.\n",
    "  - `learning_rate`: Step size for gradient updates (default: 0.01).\n",
    "- **Weights and Biases:**\n",
    "  - `Wxh`: Weight matrix for input-to-hidden connections.\n",
    "  - `Whh`: Weight matrix for hidden-to-hidden connections.\n",
    "  - `Why`: Weight matrix for hidden-to-output connections.\n",
    "  - `bh`: Bias for hidden layer.\n",
    "  - `by`: Bias for output layer.\n",
    "\n",
    "**Example Initialization:**\n",
    "```python\n",
    "rnn = SimpleRNN(input_size=5, hidden_size=10, output_size=3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Activation Function: `softmax`**\n",
    "The `softmax` function converts raw scores into probabilities for multi-class classification.\n",
    "\n",
    "- Formula:\n",
    "$$\n",
    "  \\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Forward Pass: `forward(X)`**\n",
    "This method processes a sequence of inputs through the RNN:\n",
    "- **Inputs:**\n",
    "  - `X`: Input tensor of shape `(batch_size, sequence_length, input_size)`.\n",
    "- **Process:**\n",
    "  - Loops through each time step in the sequence.\n",
    "  - Updates the hidden state using the formula:\n",
    "    $$\n",
    "    h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "    $$\n",
    "  - At the end of the sequence, the output is computed from the last hidden state:\n",
    "    $$\n",
    "    y = \\text{softmax}(W_{hy} \\cdot h_T + b_y)\n",
    "    $$\n",
    "\n",
    "**Outputs:**\n",
    "- `y`: Predictions of shape `(batch_size, output_size)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Backward Pass: `backward(X, y_true, y_pred)`**\n",
    "Implements the backpropagation through time (BPTT) algorithm:\n",
    "- **Inputs:**\n",
    "  - `X`: Input sequence of shape `(batch_size, sequence_length, input_size)`.\n",
    "  - `y_true`: True labels (one-hot encoded) of shape `(batch_size, output_size)`.\n",
    "  - `y_pred`: Predicted output from the forward pass.\n",
    "- **Steps:**\n",
    "  - Computes gradients for weights (`Wxh`, `Whh`, `Why`) and biases (`bh`, `by`).\n",
    "  - Applies gradient clipping to prevent exploding gradients.\n",
    "  - Updates weights using the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Training Loop: `train(X, y, epochs)`**\n",
    "Trains the RNN over multiple epochs:\n",
    "- **Inputs:**\n",
    "  - `X`: Input data of shape `(batch_size, sequence_length, input_size)`.\n",
    "  - `y`: True labels of shape `(batch_size, output_size)`.\n",
    "  - `epochs`: Number of training iterations.\n",
    "- **Process:**\n",
    "  - For each epoch:\n",
    "    - Calls `forward` to compute predictions.\n",
    "    - Computes the cross-entropy loss:\n",
    "      $$\n",
    "      \\text{Loss} = -\\frac{1}{N} \\sum (y \\cdot \\log(y_{\\text{pred}} + \\epsilon))\n",
    "      $$\n",
    "    - Calls `backward` to adjust parameters.\n",
    "    - Logs the loss and weight norms for debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Prediction: `predict(X)`**\n",
    "Generates predictions for a given sequence:\n",
    "- **Input:**\n",
    "  - `X`: Input sequence of shape `(batch_size, sequence_length, input_size)`.\n",
    "- **Process:**\n",
    "  - Similar to the forward pass but computes only the output from the last hidden state.\n",
    "- **Output:**\n",
    "  - `y`: Predicted probabilities of shape `(batch_size, output_size)`.\n",
    "\n",
    "---\n",
    "\n",
    "## Debugging Features\n",
    "Throughout the implementation, various print statements help debug shapes and monitor weight updates:\n",
    "- Logs intermediate shapes of input, hidden states, and outputs.\n",
    "- Prints weight norms to check for potential gradient explosion or vanishing issues.\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points to Improve\n",
    "1. **Performance:** Add support for batch processing on GPUs (e.g., via PyTorch or TensorFlow).\n",
    "2. **Scalability:** Extend to support multiple layers and bidirectional RNNs.\n",
    "3. **Numerical Stability:** Implement additional techniques to prevent overflows in `tanh` and `softmax`.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15070b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        self.Wxh = np.random.randn(self.input_size, self.hidden_size) * 0.01  # Input to hidden\n",
    "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01  # Hidden to hidden\n",
    "        self.Why = np.random.randn(self.hidden_size, self.output_size) * 0.01  # Hidden to output\n",
    "        self.bh = np.zeros((1, self.hidden_size))  # Hidden bias\n",
    "        self.by = np.zeros((1, self.output_size))  # Output bias\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for the RNN.\n",
    "        X: Input data of shape (batch_size, sequence_length, input_size).\n",
    "        Returns:\n",
    "            y: Output predictions of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        self.h_states = []  # To store hidden states for backpropagation\n",
    "        batch_size, sequence_length, input_size = X.shape\n",
    "        h_prev = np.zeros((batch_size, self.hidden_size))  # Initialize hidden state\n",
    "\n",
    "        # Iterate over each timestep in the sequence\n",
    "        for t in range(sequence_length):\n",
    "            x_t = X[:, t, :]  # Input at timestep t (shape: (batch_size, input_size))\n",
    "            h_prev = np.tanh(np.dot(x_t, self.Wxh) + np.dot(h_prev, self.Whh) + self.bh)  # Update hidden state\n",
    "            self.h_states.append(h_prev)  # Save hidden state\n",
    "\n",
    "        # Compute the output based on the last hidden state\n",
    "        y = self.softmax(np.dot(h_prev, self.Why) + self.by)\n",
    "\n",
    "        \n",
    "        # Debugging\n",
    "        print(\"X shape:\", X.shape)\n",
    "        print(\"Wxh shape:\", self.Wxh.shape)\n",
    "        print(\"Hidden state shape:\", h_prev.shape)\n",
    "        print(\"Output shape:\", y.shape)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        T = len(self.h_states)  # Total time steps\n",
    "        dh_next = np.zeros_like(self.h_states[0])\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "        # Output gradient\n",
    "        dy = y_pred - y_true\n",
    "        dWhy += np.dot(self.h_states[-1].T, dy)\n",
    "        dby += np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "        # Backprop through time\n",
    "        for t in reversed(range(T)):\n",
    "            dh = np.dot(dy, self.Why.T) + dh_next\n",
    "            dh_raw = (1 - self.h_states[t] ** 2) * dh  # Derivative through tanh\n",
    "            dbh += np.sum(dh_raw, axis=0, keepdims=True)\n",
    "            dWxh += np.dot(X[:, t, :].T, dh_raw)\n",
    "            dWhh += np.dot(self.h_states[t - 1].T, dh_raw) if t > 0 else 0\n",
    "            dh_next = np.dot(dh_raw, self.Whh.T)\n",
    "\n",
    "        # Clip gradients\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.Wxh -= self.learning_rate * dWxh\n",
    "        self.Whh -= self.learning_rate * dWhh\n",
    "        self.Why -= self.learning_rate * dWhy\n",
    "        self.bh -= self.learning_rate * dbh\n",
    "        self.by -= self.learning_rate * dby\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch}, Starting Forward\")\n",
    "            y_pred = self.forward(X)\n",
    "\n",
    "            loss = -np.sum(y * np.log(y_pred + 1e-8)) / y.shape[0]  # Add epsilon for numerical stability\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "            print(f\"Epoch {epoch}, Before Backward\")\n",
    "            self.backward(X, y, y_pred)\n",
    "            print(f\"Epoch {epoch}, After Backward\")\n",
    "\n",
    "            # Log weights for debugging\n",
    "            print(\"Weights Norms:\")\n",
    "            print(f\"Wxh: {np.linalg.norm(self.Wxh)}, Whh: {np.linalg.norm(self.Whh)}, Why: {np.linalg.norm(self.Why)}\")\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained RNN model.\n",
    "        X: Input data of shape (batch_size, sequence_length, input_size)\n",
    "        \"\"\"\n",
    "        # Forward pass through the RNN (no need for backprop)\n",
    "        h_prev = np.zeros((X.shape[0], self.hidden_size))  # Initialize hidden state\n",
    "        for t in range(X.shape[1]):  # Loop through each time step in the sequence\n",
    "            h_prev = np.tanh(np.dot(X[:, t, :], self.Wxh) + np.dot(h_prev, self.Whh) + self.bh)  # Single step\n",
    "        y = self.softmax(np.dot(h_prev, self.Why) + self.by)  # Output prediction\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fc015",
   "metadata": {},
   "source": [
    "# Chatbot with RNN for Intent Recognition\n",
    "\n",
    "The following part of notebook demonstrates how to build an intent recognition chatbot using a custom Recurrent Neural Network (RNN). The dataset is loaded from a JSON file containing intents, and the model predicts the corresponding intent for a given user input.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Dataset Loading and Preprocessing**\n",
    "\n",
    "The dataset (`intents2.json`) is structured with patterns (input phrases) and tags (labels). Here's how the data is prepared:\n",
    "\n",
    "### **Loading the Dataset**\n",
    "- The JSON file is read and parsed using Python's `json` library.\n",
    "- **Data Structure:**\n",
    "  ```json\n",
    "  {\n",
    "      \"intents\": [\n",
    "          {\n",
    "              \"tag\": \"greeting\",\n",
    "              \"patterns\": [\"Hello\", \"Hi there\", \"Good morning\"],\n",
    "              \"responses\": [\"Hi!\", \"Hello! How can I assist you?\"]\n",
    "          },\n",
    "          ...\n",
    "      ]\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Load the intents.json dataset\n",
    "with open('C:/Users/nisha/Downloads/CHATBOT-20241115T051416Z-001/CHATBOT/intents2.json', encoding='utf-8') as data_file:\n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a782f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Intent Data Preprocessing**\n",
    "\n",
    "This section of code processes the intent dataset to prepare it for training a neural network. It extracts patterns (input phrases) and tags (labels) from the dataset, tokenizes the text data, and formats the inputs and outputs appropriately for the model.\n",
    "\n",
    "### **1. Extract Patterns and Tags**\n",
    "- **Purpose:** Extract input phrases (`patterns`) and their corresponding intent labels (`tags`) from the JSON data structure.\n",
    "- **Implementation:**\n",
    "  - Iterate through each intent in `data[\"intents\"]`.\n",
    "  - Append each pattern (user input example) to the `patterns` list.\n",
    "  - Append the corresponding tag (intent label) to the `tags` list.\n",
    "\n",
    "### **2. Encode Tags**\n",
    "- **Purpose:** Convert the textual intent labels into numerical format for the model.\n",
    "- **Implementation:**\n",
    "  - Use `LabelEncoder` to transform the `tags` list into a sequence of unique integers.\n",
    "  - Each integer corresponds to a specific tag, ensuring compatibility with machine learning models.\n",
    "\n",
    "### **3. Tokenize Patterns**\n",
    "- **Purpose:** Convert text patterns into numerical sequences.\n",
    "- **Implementation:**\n",
    "  - Initialize a `Tokenizer` from TensorFlow/Keras.\n",
    "  - Fit the tokenizer on the `patterns` list, creating a word index (mapping of words to integers).\n",
    "  - Convert each pattern into a sequence of integers using the tokenizer.\n",
    "\n",
    "### **4. Pad Sequences**\n",
    "- **Purpose:** Ensure all sequences have the same length by padding shorter sequences with zeros.\n",
    "- **Implementation:**\n",
    "  - Calculate the `sequence_length` as the length of the longest sequence.\n",
    "  - Pad each sequence with zeros up to the `sequence_length` using NumPy's `np.pad` function.\n",
    "  - Result: A NumPy array of padded sequences with consistent dimensions.\n",
    "\n",
    "### **5. One-Hot Encode Labels**\n",
    "- **Purpose:** Convert the encoded intent labels into one-hot vectors for multi-class classification.\n",
    "- **Implementation:**\n",
    "  - Use NumPy to create an identity matrix of size `num_classes` (number of unique intent tags).\n",
    "  - Map each encoded label to its corresponding one-hot vector.\n",
    "\n",
    "### **6. Reshape Input Data**\n",
    "- **Purpose:** Format the input sequences (`X`) to match the input requirements of the RNN model.\n",
    "- **Implementation:**\n",
    "  - Reshape the padded sequences to have dimensions `(num_samples, 1, sequence_length)`, where:\n",
    "    - `num_samples`: Number of input patterns.\n",
    "    - `1`: Single time-step (as sequences are treated as a single input for this RNN).\n",
    "    - `sequence_length`: Length of the padded sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Output:**\n",
    "- `X`: Preprocessed input data (shape: `(num_samples, 1, sequence_length)`).\n",
    "- `y`: One-hot encoded output labels (shape: `(num_samples, num_classes)`).\n",
    "\n",
    "This preprocessing ensures that the input data is tokenized, padded, and properly formatted for training the RNN model, while the output labels are one-hot encoded for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "tags = []\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)\n",
    "        tags.append(intent[\"tag\"])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_tags = label_encoder.fit_transform(tags)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(patterns)\n",
    "sequence_data = tokenizer.texts_to_sequences(patterns)\n",
    "\n",
    "sequence_length = max(len(seq) for seq in sequence_data)\n",
    "padded_sequences = np.array([np.pad(seq, (0, sequence_length - len(seq)), mode='constant') for seq in sequence_data])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y = np.eye(num_classes)[encoded_tags]\n",
    "\n",
    "X = padded_sequences.reshape(padded_sequences.shape[0], 1, padded_sequences.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f2cb3",
   "metadata": {},
   "source": [
    "### **Description of the Code**\n",
    "\n",
    "This part of the code initializes a simple recurrent neural network (RNN) for training on the preprocessed intent dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Define Input Parameters**\n",
    "- **`input_size`:**  \n",
    "  - Represents the number of features in the input data.\n",
    "  - Set to `X.shape[2]`, which is the size of each padded sequence (sequence length). Each sequence is treated as a feature vector for the RNN.\n",
    "  \n",
    "- **`hidden_size`:**  \n",
    "  - The number of hidden units (neurons) in the RNN's hidden layer.\n",
    "  - Set to `8`, meaning the RNN will use 8 hidden units to learn and represent the temporal dependencies in the input data.\n",
    "\n",
    "- **`output_size`:**  \n",
    "  - Represents the number of classes (unique intent tags) in the output.\n",
    "  - Set to `num_classes`, which corresponds to the number of one-hot encoded labels.\n",
    "\n",
    "- **`learning_rate`:**  \n",
    "  - The step size for updating the model's weights during training.\n",
    "  - Set to `0.01`, controlling how quickly the model adapts to the training data.\n",
    "\n",
    "- **`epochs`:**  \n",
    "  - The number of iterations for training the RNN on the dataset.\n",
    "  - Set to `5000`, allowing the model sufficient iterations to minimize the error and improve predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Initialize the RNN**\n",
    "- **`SimpleRNN(input_size, hidden_size, output_size, learning_rate)`**  \n",
    "  - Creates an instance of the `SimpleRNN` class with the defined parameters.\n",
    "  - **Input:**  \n",
    "    - `input_size`: Size of the input feature vector.\n",
    "    - `hidden_size`: Number of hidden units.\n",
    "    - `output_size`: Number of output classes.\n",
    "    - `learning_rate`: Learning rate for gradient descent.\n",
    "  - **Output:**  \n",
    "    - An RNN model ready for training and predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose:**\n",
    "This code defines the architecture and hyperparameters of the RNN, preparing it to process the input data and classify the patterns into the corresponding intent tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN\n",
    "input_size = X.shape[2]\n",
    "hidden_size = 8\n",
    "output_size = num_classes\n",
    "learning_rate = 0.01\n",
    "epochs = 5000\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae4688",
   "metadata": {},
   "source": [
    "### **Description of the Code**\n",
    "\n",
    "This line of code trains the recurrent neural network (RNN) model on the prepared dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Details**\n",
    "\n",
    "- **`rnn.train(X, y, epochs=epochs)`**  \n",
    "  - **Purpose:**  \n",
    "    - Invokes the `train` method of the `SimpleRNN` class to optimize the model's parameters (weights and biases) using the training data.\n",
    "  \n",
    "  - **Parameters:**\n",
    "    - **`X`:**  \n",
    "      - The input data, formatted as `(num_samples, 1, sequence_length)`.  \n",
    "      - Represents the tokenized and padded user inputs.  \n",
    "    - **`y`:**  \n",
    "      - The target labels, formatted as one-hot encoded vectors of shape `(num_samples, num_classes)`.  \n",
    "      - Represents the intent categories associated with each input sequence.\n",
    "    - **`epochs`:**  \n",
    "      - The number of iterations for training.  \n",
    "      - Set to `5000`, allowing the RNN to repeatedly update its parameters to minimize the loss.\n",
    "\n",
    "  - **Process:**  \n",
    "    - **Forward Pass:**  \n",
    "      - Computes the predictions for the input sequences by propagating data through the network.  \n",
    "    - **Loss Calculation:**  \n",
    "      - Calculates the difference between the predicted output and the actual labels (`y`).  \n",
    "      - Uses categorical cross-entropy as the loss function for multi-class classification.  \n",
    "    - **Backward Pass:**  \n",
    "      - Computes gradients with respect to the loss using backpropagation through time (BPTT).  \n",
    "      - Updates the model parameters using gradient descent with the specified `learning_rate`.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Outcome**\n",
    "- Optimizes the RNN's parameters to reduce the classification error over `5000` training epochs.\n",
    "- At the end of training, the RNN will be better equipped to predict the intent labels for new user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.train(X, y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ef7e9",
   "metadata": {},
   "source": [
    "### **Description of the Code**\n",
    "\n",
    "The `predict_intent` function is responsible for predicting the intent of a user input using the trained RNN model. It processes raw user input into a format suitable for the RNN, performs predictions, and maps the predicted class to the corresponding intent tag.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Explanation**\n",
    "\n",
    "1. **Function Definition and Parameters:**\n",
    "   - **`user_input`:** The raw text input from the user (e.g., \"Hello, how are you?\").\n",
    "   - **`tokenizer`:** A tokenizer object that converts text into numerical sequences based on the training vocabulary.\n",
    "   - **`rnn`:** The trained `SimpleRNN` model used to make predictions.\n",
    "   - **`label_encoder`:** An encoder that maps numerical labels to their corresponding intent tags.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Tokenize the Input:**\n",
    "   ```python\n",
    "   input_sequence = [tokenizer.word_index.get(word.lower(), 0) for word in user_input.split()]\n",
    "   ```\n",
    "   - Splits the `user_input` into individual words.\n",
    "   - Converts each word into its corresponding token (numerical representation) using the `tokenizer.word_index`.\n",
    "   - If a word is not found in the training vocabulary, it is assigned the default token `0`.\n",
    "   - **Example:**  \n",
    "     Input: `\"Hello bot\"` → Tokenized: `[3, 7]`.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Pad the Sequence:**\n",
    "   ```python\n",
    "   input_sequence = np.pad(input_sequence, (0, sequence_length - len(input_sequence)), mode='constant')[:sequence_length]\n",
    "   ```\n",
    "   - Ensures the tokenized input matches the `sequence_length` used during training.\n",
    "   - Pads shorter sequences with zeros at the end.\n",
    "   - Truncates sequences longer than `sequence_length`.\n",
    "   - **Example:**  \n",
    "     Tokenized: `[3, 7]` → Padded: `[3, 7, 0, 0]` (assuming `sequence_length = 4`).\n",
    "\n",
    "---\n",
    "\n",
    "4. **Reshape for RNN Input:**\n",
    "   ```python\n",
    "   input_sequence = np.array([input_sequence]).reshape(1, 1, -1)\n",
    "   ```\n",
    "   - Reshapes the padded input sequence to the required shape for the RNN: `(batch_size, 1, sequence_length)`.\n",
    "   - **Example:**  \n",
    "     Padded: `[3, 7, 0, 0]` → Reshaped: `(1, 1, 4)`.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Make Predictions:**\n",
    "   ```python\n",
    "   predictions = rnn.predict(input_sequence)\n",
    "   ```\n",
    "   - Passes the reshaped input through the trained RNN to generate predictions for each intent class.\n",
    "   - **`predictions`:** A probability distribution over all classes (e.g., `[0.1, 0.6, 0.3]`).\n",
    "\n",
    "6. **Determine Predicted Class:**\n",
    "   ```python\n",
    "   predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "   ```\n",
    "   - Finds the class index with the highest predicted probability (e.g., index `1`).\n",
    "\n",
    "7. **Decode the Predicted Tag:**\n",
    "   ```python\n",
    "   predicted_tag = label_encoder.inverse_transform([predicted_class])[0]\n",
    "   ```\n",
    "   - Maps the predicted class index back to the corresponding intent tag using the `label_encoder`.\n",
    "   - **Example:**  \n",
    "     Class index `1` → Intent tag `\"greeting\"`.\n",
    "\n",
    "---\n",
    "\n",
    "8. **Return Predicted Tag:**\n",
    "   ```python\n",
    "   return predicted_tag\n",
    "   ```\n",
    "   - Returns the final intent tag for the user's input.\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose:**\n",
    "The `predict_intent` function takes a raw user input, processes it, and returns the predicted intent tag, enabling the chatbot to understand user queries and respond appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(user_input, tokenizer, rnn, label_encoder):\n",
    "    # Convert user input to tokenized sequence\n",
    "    input_sequence = [tokenizer.word_index.get(word.lower(), 0) for word in user_input.split()]\n",
    "    print(f\"Tokenized Input: {input_sequence}\")  # Debugging\n",
    "\n",
    "    # Pad the sequence\n",
    "    input_sequence = np.pad(input_sequence, (0, sequence_length - len(input_sequence)), mode='constant')[:sequence_length]\n",
    "    print(f\"Padded Input: {input_sequence}\")  # Debugging\n",
    "\n",
    "    # Reshape for RNN\n",
    "    input_sequence = np.array([input_sequence]).reshape(1, 1, -1)\n",
    "    print(f\"Input Shape for RNN: {input_sequence.shape}\")  # Debugging\n",
    "\n",
    "    # Predict class\n",
    "    predictions = rnn.predict(input_sequence)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    # Decode the predicted tag\n",
    "    predicted_tag = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    print(f\"Predicted Tag: {predicted_tag}\")  # Debugging\n",
    "    return predicted_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d37afa",
   "metadata": {},
   "source": [
    "### **Description of the `predict` Method**\n",
    "\n",
    "The `predict` method in this code is used to make predictions on input sequences after the Recurrent Neural Network (RNN) model has been trained. The method takes an input sequence, passes it through the RNN's layers, and outputs the predicted probabilities for each class.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Breakdown:**\n",
    "\n",
    "1. **Input Parameter:**\n",
    "   - **`X`:** The input data with shape `(batch_size, sequence_length, input_size)`. This is the padded and tokenized input sequence passed to the RNN for prediction.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Initialize Hidden State:**\n",
    "   ```python\n",
    "   h_prev = np.zeros((X.shape[0], self.hidden_size))\n",
    "   ```\n",
    "   - Initializes the hidden state vector `h_prev` to zeros with shape `(batch_size, hidden_size)`. This represents the initial state of the RNN before processing the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Iterate Over Time Steps:**\n",
    "   ```python\n",
    "   for t in range(X.shape[1]):\n",
    "       h_prev = np.tanh(np.dot(X[:, t, :], self.Wxh) + np.dot(h_prev, self.Whh) + self.bh)\n",
    "   ```\n",
    "   - **Loop over each time step (t) of the input sequence**: The loop runs for `sequence_length` iterations (the second dimension of `X`).\n",
    "   - **Input at Time `t`**: `X[:, t, :]` represents the input at time `t` (shape: `(batch_size, input_size)`).\n",
    "   - **Hidden State Update**:  \n",
    "     The hidden state is updated using the following equation:\n",
    "     \\[\n",
    "     h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "     \\]\n",
    "     where:\n",
    "     - `Wxh`: Weight matrix from input to hidden layer.\n",
    "     - `Whh`: Weight matrix from hidden state at time `t-1` to hidden state at time `t`.\n",
    "     - `bh`: Bias term for the hidden layer.\n",
    "     - `tanh`: The activation function, which introduces non-linearity.\n",
    "     - **Output**: `h_prev` is updated to store the current hidden state at time `t`.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Compute Output:**\n",
    "   ```python\n",
    "   y = self.softmax(np.dot(h_prev, self.Why) + self.by)\n",
    "   ```\n",
    "   - Once the loop has processed all time steps, the final hidden state `h_prev` (representing the learned information from the entire sequence) is passed through the output layer.\n",
    "   - The output is computed as:\n",
    "     \\[\n",
    "     y = \\text{softmax}(h_T \\cdot W_{hy} + b_y)\n",
    "     \\]\n",
    "     where:\n",
    "     - `Why`: Weight matrix from hidden state to output.\n",
    "     - `by`: Output bias.\n",
    "     - The softmax function is applied to ensure that the output represents a probability distribution over the possible classes.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Debugging Output:**\n",
    "   ```python\n",
    "   print(f\"Predictions: {y}\")  # Debugging\n",
    "   ```\n",
    "   - Prints the output predictions (probability distribution) for debugging purposes.\n",
    "   - **Example Output:**  \n",
    "     `Predictions: [[0.2, 0.5, 0.3]]` - This shows the probability distribution across the classes.\n",
    "\n",
    "---\n",
    "\n",
    "6. **Return Predicted Output:**\n",
    "   ```python\n",
    "   return y\n",
    "   ```\n",
    "   - The method returns the output `y`, which is a probability distribution over all possible classes (i.e., the RNN's prediction for the input sequence).\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose:**\n",
    "The `predict` method performs the forward pass of the RNN for a given input sequence, processes it through the network layers, and outputs the predicted probabilities for each class. It enables the RNN to make predictions on unseen data, such as classifying the intent of a user input in a chatbot scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    h_prev = np.zeros((X.shape[0], self.hidden_size))\n",
    "    for t in range(X.shape[1]):\n",
    "        h_prev = np.tanh(np.dot(X[:, t, :], self.Wxh) + np.dot(h_prev, self.Whh) + self.bh)\n",
    "    y = self.softmax(np.dot(h_prev, self.Why) + self.by)\n",
    "    print(f\"Predictions: {y}\")  # Debugging\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31020557",
   "metadata": {},
   "source": [
    "### **Description of the `get_response` Function**\n",
    "\n",
    "The `get_response` function is designed to retrieve an appropriate response based on a predicted tag. It looks for the tag in a dataset containing different intents and their associated responses, and returns a random response from the matching intent.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Breakdown:**\n",
    "\n",
    "1. **Input Parameters:**\n",
    "   - **`tag`**: The predicted tag from the RNN, which represents a specific intent (e.g., \"greeting\", \"goodbye\", etc.).\n",
    "   - **`data`**: A dataset in JSON format containing various intents and their associated responses. This dataset includes different \"tags\" and lists of \"responses\" tied to each tag.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Iterate Over Intents:**\n",
    "   ```python\n",
    "   for intent in data[\"intents\"]:\n",
    "   ```\n",
    "   - The function loops over the list of \"intents\" in the `data` dictionary. Each intent contains a \"tag\" and a list of possible responses.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Check for Matching Tag:**\n",
    "   ```python\n",
    "   if intent[\"tag\"] == tag:\n",
    "   ```\n",
    "   - For each intent in the dataset, it checks whether the \"tag\" matches the input `tag` (the predicted intent from the RNN).\n",
    "   - If a match is found, the corresponding intent’s \"responses\" will be used.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Debugging and Response Retrieval:**\n",
    "   ```python\n",
    "   print(f\"Found Response for Tag '{tag}': {intent['responses']}\")  # Debugging\n",
    "   return np.random.choice(intent[\"responses\"])\n",
    "   ```\n",
    "   - If the `tag` matches an intent's \"tag\", the function prints the matching \"responses\" for debugging purposes.\n",
    "   - It then selects a random response from the list of responses associated with the matching intent using `np.random.choice()` and returns that response.\n",
    "\n",
    "---\n",
    "\n",
    "5. **No Matching Tag:**\n",
    "   ```python\n",
    "   print(f\"No Response Found for Tag '{tag}'\")  # Debugging\n",
    "   return \"I'm sorry, I don't understand.\"\n",
    "   ```\n",
    "   - If no matching tag is found after iterating through all intents, the function prints a message indicating that no response was found for the given tag.\n",
    "   - It then returns a default message, `\"I'm sorry, I don't understand.\"`, as a fallback when no response is found for the given tag.\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose:**\n",
    "The `get_response` function is used to generate a response based on the predicted tag. It is typically used in a chatbot or conversational AI system where the input is processed by an RNN model, the intent (tag) is predicted, and the corresponding response is retrieved from a predefined list associated with that tag.\n",
    "\n",
    "### **Example Flow:**\n",
    "- **Input:** A predicted tag (e.g., `\"greeting\"`).\n",
    "- **Process:** The function checks the dataset for the intent with the tag `\"greeting\"`, retrieves its list of responses, and returns one of those responses at random.\n",
    "- **Output:** A random greeting response from the list associated with the \"greeting\" intent, such as `\"Hello! How can I help you?\"`. If no tag matches, it returns a default response like `\"I'm sorry, I don't understand.\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(tag, data):\n",
    "    for intent in data[\"intents\"]:\n",
    "        if intent[\"tag\"] == tag:\n",
    "            print(f\"Found Response for Tag '{tag}': {intent['responses']}\")  # Debugging\n",
    "            return np.random.choice(intent[\"responses\"])\n",
    "    print(f\"No Response Found for Tag '{tag}'\")  # Debugging\n",
    "    return \"I'm sorry, I don't understand.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aad580",
   "metadata": {},
   "source": [
    "### **Description of the Chatbot Interaction Code**\n",
    "\n",
    "The provided code snippet simulates a simple chatbot that interacts with a user. The chatbot responds to user inputs based on the model's predictions of intent tags, and provides relevant responses from a predefined dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Breakdown:**\n",
    "\n",
    "1. **Initial Greeting:**\n",
    "   ```python\n",
    "   print(\"Chatbot: Hi! Type 'quit' to exit.\")\n",
    "   ```\n",
    "   - This line prints an initial greeting from the chatbot to welcome the user and inform them that they can type \"quit\" to end the conversation.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Infinite Loop for Interaction:**\n",
    "   ```python\n",
    "   while True:\n",
    "   ```\n",
    "   - The chatbot interaction is enclosed in a `while True` loop, which means the chatbot will continuously accept user input until the user types `'quit'` to break out of the loop and end the conversation.\n",
    "\n",
    "---\n",
    "\n",
    "3. **User Input:**\n",
    "   ```python\n",
    "   user_input = input(\"You: \").strip()\n",
    "   ```\n",
    "   - The `input()` function is used to collect input from the user. The `.strip()` method is applied to remove any leading or trailing spaces in the user's input.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Check for Exit Condition:**\n",
    "   ```python\n",
    "   if user_input.lower() == \"quit\":\n",
    "       print(\"Chatbot: Goodbye!\")\n",
    "       break\n",
    "   ```\n",
    "   - The chatbot checks if the user's input is `\"quit\"`, regardless of case (because of `.lower()`). If the user types `\"quit\"`, the chatbot responds with `\"Goodbye!\"` and the `break` statement terminates the loop, ending the conversation.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Predict the Intent:**\n",
    "   ```python\n",
    "   predicted_tag = predict_intent(user_input, tokenizer, rnn, label_encoder)\n",
    "   ```\n",
    "   - If the user does not type `\"quit\"`, the chatbot proceeds to predict the intent of the user's input using the `predict_intent()` function. This function tokenizes the user input, processes it through the RNN model, and returns the predicted tag (e.g., `\"greeting\"`, `\"goodbye\"`).\n",
    "\n",
    "---\n",
    "\n",
    "6. **Retrieve Bot Response:**\n",
    "   ```python\n",
    "   bot_response = get_response(predicted_tag, data)\n",
    "   ```\n",
    "   - The predicted intent tag is passed to the `get_response()` function, which retrieves an appropriate response from the `data` (the dataset of intents and their associated responses). The response is based on the tag predicted by the RNN.\n",
    "\n",
    "---\n",
    "\n",
    "7. **Display Bot Response:**\n",
    "   ```python\n",
    "   print(f\"Chatbot: {bot_response}\")\n",
    "   ```\n",
    "   - The chatbot then prints the response (`bot_response`) it has generated, simulating a conversation with the user.\n",
    "\n",
    "---\n",
    "\n",
    "### **Flow of the Chatbot Interaction:**\n",
    "\n",
    "1. **Initial Interaction:**\n",
    "   - The chatbot greets the user: `\"Chatbot: Hi! Type 'quit' to exit.\"`\n",
    "   \n",
    "2. **User Input:**\n",
    "   - The user enters their input, and the chatbot processes the input through the following steps:\n",
    "     - Predicts the intent based on the user's input.\n",
    "     - Retrieves a matching response based on the predicted intent tag.\n",
    "   \n",
    "3. **Bot Response:**\n",
    "   - The chatbot responds to the user with a relevant reply based on the intent prediction.\n",
    "\n",
    "4. **Loop Continuation or Exit:**\n",
    "   - The chatbot continues the interaction until the user types `\"quit\"`, at which point the conversation ends with the message `\"Chatbot: Goodbye!\"`.\n",
    "\n",
    "---\n",
    "\n",
    "This simple flow allows for continuous conversation until the user decides to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot interaction\n",
    "print(\"Chatbot: Hi! Type 'quit' to exit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    predicted_tag = predict_intent(user_input, tokenizer, rnn, label_encoder)\n",
    "    bot_response = get_response(predicted_tag, data)\n",
    "    print(f\"Chatbot: {bot_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
