{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c736af1e",
   "metadata": {},
   "source": [
    "### Chatbot Interaction with Multiple Models\n",
    "\n",
    "This section of the code defines the `chat()` function, which allows users to interact with the chatbot using a trained model. The function loads the pre-trained model, tokenizer, and label encoder, and processes user input to provide an appropriate response based on the model's prediction.\n",
    "\n",
    "#### Key Components:\n",
    "1. **Model Loading**:\n",
    "   - The function accepts a `model_type` parameter (e.g., 'LSTM', 'RNN', 'BRNN', or 'Encoder-Decoder'), which determines which pre-trained model to load.\n",
    "   - The model is loaded using `keras.models.load_model()` by dynamically specifying the model filename based on the selected model type (e.g., `RNN_chat_model.h5`).\n",
    "\n",
    "2. **Tokenizer and Label Encoder Loading**:\n",
    "   - The tokenizer object is loaded from the `tokenizer.pickle` file using the `pickle` library. This tokenizer is used to convert user input into sequences of tokens that the model can process.\n",
    "   - The label encoder object is also loaded from `label_encoder.pickle`, which helps in converting the model's predicted tag (integer) back into the corresponding intent label (string).\n",
    "\n",
    "3. **User Interaction**:\n",
    "   - The chatbot enters an interactive loop where it continually accepts user input (via `input()`).\n",
    "   - If the user types \"quit\", the loop ends and the program exits.\n",
    "\n",
    "4. **Processing User Input**:\n",
    "   - The user's input is tokenized using `tokenizer.texts_to_sequences()`, which converts the input text into a sequence of integers (tokens).\n",
    "   - The sequence is then padded to a fixed length (20 tokens in this case) using `keras.preprocessing.sequence.pad_sequences()`, ensuring uniform input size for the model.\n",
    "\n",
    "5. **Model Prediction**:\n",
    "   - The padded input sequence is passed through the trained model for prediction (`model.predict()`).\n",
    "   - The model outputs a probability distribution for each tag. The predicted tag is the one with the highest probability, which is then converted back to a human-readable label using the label encoder (`lbl_encoder.inverse_transform()`).\n",
    "\n",
    "6. **Response Generation**:\n",
    "   - Based on the predicted tag, the function iterates through the `data['intents']` and finds the corresponding intent. \n",
    "   - It randomly selects one of the predefined responses for that intent and prints it as the chatbot's reply.\n",
    "\n",
    "7. **Color and Formatting**:\n",
    "   - The `colorama` library is used for adding color to the console output. \n",
    "     - User input is displayed in light blue, and chatbot responses are shown in green.\n",
    "     - The message \"Start messaging with the bot (type quit to stop)!\" is printed in yellow to indicate the start of the interaction.\n",
    "\n",
    "#### Example Usage:\n",
    "To use a specific model, the `chat()` function is called with the desired model name:\n",
    "- `chat(model_type='RNN')` will load and use the RNN model.\n",
    "- Similarly, you can pass 'LSTM', 'BRNN', or 'Encoder-Decoder' to use the respective models.\n",
    "\n",
    "### Summary:\n",
    "This code allows users to interact with the chatbot using any of the pre-trained models (LSTM, RNN, BRNN, or Encoder-Decoder). It processes user input, predicts the intent, and provides a relevant response based on the model's prediction. The flexibility to choose between different models makes it easy to evaluate and compare the performance of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a2caa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start messaging with the bot (type quit to stop)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 166ms/st ━━━━━━━━━━━━━━━━━━━━ 0s 187ms/step\n",
      "ChatBot: hi there, how can i help you\n",
      "User: reservation\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/ste ━━━━━━━━━━━━━━━━━━━━ 0s 53ms/step\n",
      "ChatBot: hello thanks for checking in, how can i help you\n",
      "User: tax\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/ste ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step\n",
      "ChatBot: hello thanks for checking in, how can i help you\n",
      "User: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 52>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(Fore\u001b[38;5;241m.\u001b[39mYELLOW \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart messaging with the bot (type quit to stop)!\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m Style\u001b[38;5;241m.\u001b[39mRESET_ALL)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# To run with a specific model, pass the model name ('LSTM', 'RNN', 'BRNN', or 'Encoder-Decoder')\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mchat\u001b[1;34m(model_type)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Fore\u001b[38;5;241m.\u001b[39mLIGHTBLUE_EX \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m Style\u001b[38;5;241m.\u001b[39mRESET_ALL, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Accept user input\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import colorama \n",
    "colorama.init()\n",
    "from colorama import Fore, Style\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Load the intents data\n",
    "with open('C:/Users/nisha/Downloads/CHATBOT-20241115T051416Z-001/CHATBOT/intents2.json', encoding='utf-8') as data_file:\n",
    "    data = json.load(data_file)\n",
    "\n",
    "def chat(model_type='BRNN'):\n",
    "    # Load the trained model\n",
    "    model = keras.models.load_model(f'{model_type}_chat_model.h5')\n",
    "\n",
    "    # Load tokenizer object\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    # Load label encoder object\n",
    "    with open('label_encoder.pickle', 'rb') as enc:\n",
    "        lbl_encoder = pickle.load(enc)\n",
    "\n",
    "    # Parameters\n",
    "    max_len = 20\n",
    "    \n",
    "    while True:\n",
    "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "        inp = input()  # Accept user input\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        # Prepare the input for the model (tokenize and pad)\n",
    "        sequence = tokenizer.texts_to_sequences([inp])\n",
    "        padded_sequence = keras.preprocessing.sequence.pad_sequences(sequence, truncating='post', maxlen=max_len)\n",
    "        \n",
    "        # Predict the tag\n",
    "        result = model.predict(padded_sequence)\n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "        # Find and display the response based on the predicted tag\n",
    "        for i in data['intents']:\n",
    "            if i['tag'] == tag:\n",
    "                print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL , np.random.choice(i['responses']))\n",
    "\n",
    "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
    "\n",
    "# To run with a specific model, pass the model name ('LSTM', 'RNN', 'BRNN', or 'Encoder-Decoder')\n",
    "chat(model_type='RNN')  # Change 'LSTM' to 'RNN', 'BRNN', or 'Encoder-Decoder' for other models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
